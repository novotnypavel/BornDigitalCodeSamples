{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6b0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "jsonSchema = StructType([\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('tweet_id', LongType(), True),\n",
    "    StructField('tweet_text', StringType(), True)\n",
    "])\n",
    "\n",
    "#replace the file path\n",
    "df=spark.read.format(\"json\").schema(jsonSchema).load(\"/Users/Pavel/Documents/KULeuven/Courses/AdvancedAnalyticsinBigDataWorld/spark/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c31664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61008"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159d5d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 42674\n",
      "Test Dataset Count: 18334\n"
     ]
    }
   ],
   "source": [
    "(train, test) = df.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305ef95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "class RegexReplacerWritable(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    #value = Param(\n",
    "    #   Params._dummy(),\n",
    "    #   \"value\",\n",
    "    #   \"value to fill\",\n",
    "    #)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(RegexReplacerWritable, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        #Converting all letters to lowercase\n",
    "        df = df.withColumn(\"tweet_text\",f.lower(f.col(\"tweet_text\")))\n",
    "        #removing punctuations, numbers, http and spaces\n",
    "        df = df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'([^ a-zA-Z\\'])',''))\n",
    "        df = df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'http.*?\\\\b',' '))\n",
    "        df = df.withColumn(\"tweet_text\",f.ltrim(f.regexp_replace(f.col(\"tweet_text\"),'[\\r\\n\\t\\f\\v ]+', ' ')))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15753023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class UDLemmatization(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(UDLemmatization, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        pandas_df = df.select(\"*\").toPandas()\n",
    "        pandas_df['lemmatized'] = pandas_df['words'].apply(\n",
    "                    lambda lst:[lemmatizer.lemmatize(word) for word in lst])\n",
    "        pandas_df['lemmatized']\n",
    "        df = spark.createDataFrame(pandas_df)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "m = __import__(\"__main__\"); \n",
    "setattr(m, 'UDLemmatization', UDLemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ffd76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class UDShortWordsRemover(\n",
    "    Transformer, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(UDShortWordsRemover, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this RegexReplacerWritable.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        df = df.withColumn(\"filtered2\", f.expr(\"filter(filtered, x -> not(length(x) < 3))\")).where(f.size(f.col(\"filtered2\")) > 0).drop(\"filtered\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7c15d",
   "metadata": {},
   "source": [
    "## Pipeline preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, IDF, HashingTF, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# 1. Regex Filter replacer\n",
    "regexrep = RegexReplacerWritable()\n",
    "\n",
    "# 2. Tokenizer - splitting words \n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "\n",
    "# 3. Lemmatizer user defined\n",
    "lemmatizerUD = UDLemmatization()\n",
    "\n",
    "# 4. Stop Words Remover\n",
    "stopwordList = [\"u\",\"ur\", \"amp\", \"q\"] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\" ,stopWords=stopwordList)\n",
    "\n",
    "# 5. Short Words len <Â 3 user defined remover \n",
    "shortWordsremover = UDShortWordsRemover()\n",
    "\n",
    "# 6. Count Vectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered2\", outputCol=\"features\")\n",
    "\n",
    "# 7. IDF\n",
    "idf = IDF(inputCol = \"features\", outputCol = \"tf_idf_features\")\n",
    "\n",
    "# 8. String Indexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"labelIndex\")\n",
    "\n",
    "# 9. Logistic Regression\n",
    "lr = LogisticRegression(labelCol = \"labelIndex\", featuresCol = \"tf_idf_features\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# 10. Index to String, for now labels only, not prediction - TODO\n",
    "converter = IndexToString(inputCol=\"labelIndex\", outputCol=\"labelOriginal\")\n",
    "\n",
    "#create the pipeline\n",
    "pipeline = Pipeline(stages=[regexrep, tokenizer, lemmatizerUD, remover, shortWordsremover, cv, idf, label_stringIdx, lr, converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40572fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22dedc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFitTrain = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f008e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipelineFitTrain.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85648ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFitTrain.write().overwrite().save('lr_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4775893",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7de7413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6465093411996067\n",
      "Test Error = 0.353491 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(accuracy)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d00515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Create both evaluators\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\")\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = prediction.select(\"prediction\", \"labelIndex\")\n",
    "\n",
    "# Get metrics\n",
    "acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "auc = evaluatorMulti.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23069f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.6465093411996067\n",
      "F1 score: 0.6467342949460959\n",
      "Weighted Precision: 0.6731761558628765\n",
      "Weighted Recall: 0.6465093411996066\n",
      "AUC: 0.6467342949460959\n"
     ]
    }
   ],
   "source": [
    "print(\"ACC: %s\" % acc)\n",
    "print(\"F1 score: %s\" % f1)\n",
    "print(\"Weighted Precision: %s\" % weightedPrecision)\n",
    "print(\"Weighted Recall: %s\" % weightedRecall)\n",
    "print(\"AUC: %s\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c44d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 0.05860863540452871\n",
      "label 1: 0.028397693079237712\n",
      "label 2: 0.008277703604806409\n",
      "label 3: 0.0061695800794819775\n",
      "label 4: 0.00062940914216779\n",
      "label 5: 2.4047710657945365e-05\n",
      "True positive rate by label:\n",
      "label 0: 0.9652822543512294\n",
      "label 1: 0.9135225999252895\n",
      "label 2: 0.9415066801392163\n",
      "label 3: 0.8971187207723639\n",
      "label 4: 0.8868556125864763\n",
      "label 5: 0.7276264591439688\n",
      "Precision by label:\n",
      "label 0: 0.8492262821032164\n",
      "label 1: 0.9152320359281437\n",
      "label 2: 0.967801500288517\n",
      "label 3: 0.964013616469444\n",
      "label 4: 0.9939969984992496\n",
      "label 5: 0.9986648865153538\n",
      "Recall by label:\n",
      "label 0: 0.9652822543512294\n",
      "label 1: 0.9135225999252895\n",
      "label 2: 0.9415066801392163\n",
      "label 3: 0.8971187207723639\n",
      "label 4: 0.8868556125864763\n",
      "label 5: 0.7276264591439688\n",
      "F-measure by label:\n",
      "label 0: 0.9035427980346521\n",
      "label 1: 0.9143765189755094\n",
      "label 2: 0.9544730252674709\n",
      "label 3: 0.9293639631192374\n",
      "label 4: 0.937374690411605\n",
      "label 5: 0.8418683173888576\n",
      "Accuracy: 0.9227212991645546\n",
      "FPR: 0.024828368185435126\n",
      "TPR: 0.9227212991645547\n",
      "F-measure: 0.9229976667284562\n",
      "Precision: 0.9272842184909433\n",
      "Recall: 0.9227212991645547\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = pipelineFitTrain.stages[8].summary\n",
    "\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
