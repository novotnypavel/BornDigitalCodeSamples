{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-37999488a5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m jsonSchema = StructType([\n\u001b[1;32m      3\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweet_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweet_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "jsonSchema = StructType([\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('tweet_id', LongType(), True),\n",
    "    StructField('tweet_text', StringType(), True)\n",
    "])\n",
    "\n",
    "#replace the file path\n",
    "df=spark.read.format(\"json\").schema(jsonSchema).load(\"/Users/Pavel/Documents/KULeuven/Courses/AdvancedAnalyticsinBigDataWorld/spark/data/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import ltrim\n",
    "\n",
    "#Converting all letters to lowercase\n",
    "df = df.withColumn(\"tweet_text\",f.lower(f.col(\"tweet_text\")))\n",
    "\n",
    "#removing punctuations, numbers, http and spaces\n",
    "df =df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'([^ a-zA-Z\\'])',''))\n",
    "df = df.withColumn(\"tweet_text\",f.regexp_replace(f.col(\"tweet_text\"),'http.*?\\\\b',' '))\n",
    "df = df.withColumn(\"tweet_text\",f.ltrim(f.regexp_replace(f.col(\"tweet_text\"),'[\\r\\n\\t\\f\\v ]+', ' ')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "#Splitting words\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "dataset = tokenizer.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "#just to try if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Didn't manage to apply lemmatization while using WordNetLemmatizer on pyspark dataframe. \n",
    "#So temporarily to make it work, firstly changed to pandas dataframe and then back to pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = dataset.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [could, be, a, messan, all, out, war, of, the,...\n",
       "1        [infectious, disease, designated, bed, drastic...\n",
       "2                         [do, u, truly, trust, safety, i]\n",
       "3                                          [do, u, use, q]\n",
       "4                   [want, but, get, amp, a, dominates, u]\n",
       "                               ...                        \n",
       "21177                                          [on, break]\n",
       "21178                                      [so, bored, of]\n",
       "21179                                           [get, the]\n",
       "21180                                         [is, boring]\n",
       "21181                                          [fuck, you]\n",
       "Name: lemmatized, Length: 21182, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pandas_df['lemmatized'] = pandas_df['words'].apply(\n",
    "                    lambda lst:[lemmatizer.lemmatize(word) for word in lst])\n",
    "pandas_df['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words\n",
    "stopwordList = [\"u\",\"ur\", \"amp\"] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "remover = StopWordsRemover(inputCol=\"lemmatized\", outputCol=\"filtered\" ,stopWords=stopwordList)\n",
    "dataset2 = remover.transform(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n",
      "|label         |tweet_id           |tweet_text                                                                                                        |words                                                                                                                             |lemmatized                                                                                                                        |filtered2                                                                                                     |\n",
      "+--------------+-------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n",
      "|#china        |1381303111222956033|could be a messan all out war of the worlds                                                                       |[could, be, a, messan, all, out, war, of, the, worlds]                                                                            |[could, be, a, messan, all, out, war, of, the, world]                                                                             |[messan, war, world]                                                                                          |\n",
      "|#vaccine      |1380298538802765827|infectious disease designated bed drastic decreasejapan                                                           |[infectious, disease, designated, bed, drastic, decreasejapan]                                                                    |[infectious, disease, designated, bed, drastic, decreasejapan]                                                                    |[infectious, disease, designated, bed, drastic, decreasejapan]                                                |\n",
      "|#vaccine      |1381494016211030016|do u truly trust safety i                                                                                         |[do, u, truly, trust, safety, i]                                                                                                  |[do, u, truly, trust, safety, i]                                                                                                  |[truly, trust, safety]                                                                                        |\n",
      "|#covid        |1380766615189479425|do u use q                                                                                                        |[do, u, use, q]                                                                                                                   |[do, u, use, q]                                                                                                                   |[use]                                                                                                         |\n",
      "|#biden        |1380129673443696640|wants but gets amp as dominates us                                                                                |[wants, but, gets, amp, as, dominates, us]                                                                                        |[want, but, get, amp, a, dominates, u]                                                                                            |[want, get, dominates]                                                                                        |\n",
      "|#stopasianhate|1380973739224010757|queen of asia                                                                                                     |[queen, of, asia]                                                                                                                 |[queen, of, asia]                                                                                                                 |[queen, asia]                                                                                                 |\n",
      "|#covid        |1381746356730101760|whats ur main form of q                                                                                           |[whats, ur, main, form, of, q]                                                                                                    |[whats, ur, main, form, of, q]                                                                                                    |[whats, main, form]                                                                                           |\n",
      "|#biden        |1380127152847347714|no one wants to buy the top on low volume are gone presaging a plunge                                             |[no, one, wants, to, buy, the, top, on, low, volume, are, gone, presaging, a, plunge]                                             |[no, one, want, to, buy, the, top, on, low, volume, are, gone, presaging, a, plunge]                                              |[one, want, buy, top, low, volume, gone, presaging, plunge]                                                   |\n",
      "|#covid        |1381347661396246531|domain for sale doge com coin style                                                                               |[domain, for, sale, doge, com, coin, style]                                                                                       |[domain, for, sale, doge, com, coin, style]                                                                                       |[domain, sale, doge, com, coin, style]                                                                        |\n",
      "|#china        |1380180497062629376|dm or comment ready or life nstyle                                                                                |[dm, or, comment, ready, or, life, nstyle]                                                                                        |[dm, or, comment, ready, or, life, nstyle]                                                                                        |[comment, ready, life, nstyle]                                                                                |\n",
      "|#china        |1380177653383237632|let's get started or life nstyle                                                                                  |[let's, get, started, or, life, nstyle]                                                                                           |[let's, get, started, or, life, nstyle]                                                                                           |[get, started, life, nstyle]                                                                                  |\n",
      "|#biden        |1380986061078282245|it was a lie our govt did it                                                                                      |[it, was, a, lie, our, govt, did, it]                                                                                             |[it, wa, a, lie, our, govt, did, it]                                                                                              |[lie, govt]                                                                                                   |\n",
      "|#biden        |1380521381704708098|new platform hyprr on udoo making noise before marketing contentdata control w better monetization amp freespeech |[new, platform, hyprr, on, udoo, making, noise, before, marketing, contentdata, control, w, better, monetization, amp, freespeech]|[new, platform, hyprr, on, udoo, making, noise, before, marketing, contentdata, control, w, better, monetization, amp, freespeech]|[new, platform, hyprr, udoo, making, noise, marketing, contentdata, control, better, monetization, freespeech]|\n",
      "|#china        |1380182109097091072|don't miss out this great opportunity or life nstyle                                                              |[don't, miss, out, this, great, opportunity, or, life, nstyle]                                                                    |[don't, miss, out, this, great, opportunity, or, life, nstyle]                                                                    |[miss, great, opportunity, life, nstyle]                                                                      |\n",
      "|#covid        |1381753369576894465|the amazing singapore at night ig united                                                                          |[the, amazing, singapore, at, night, ig, united]                                                                                  |[the, amazing, singapore, at, night, ig, united]                                                                                  |[amazing, singapore, night, united]                                                                           |\n",
      "|#china        |1381634037891047427|learn holy quran                                                                                                  |[learn, holy, quran]                                                                                                              |[learn, holy, quran]                                                                                                              |[learn, holy, quran]                                                                                          |\n",
      "|#covid        |1379865363953893378|covid update v                                                                                                    |[covid, update, v]                                                                                                                |[covid, update, v]                                                                                                                |[covid, update]                                                                                               |\n",
      "|#biden        |1380131305090875393|if is booming why are sent to americans amp pumping billionmonth into support                                     |[if, is, booming, why, are, sent, to, americans, amp, pumping, billionmonth, into, support]                                       |[if, is, booming, why, are, sent, to, american, amp, pumping, billionmonth, into, support]                                        |[booming, sent, american, pumping, billionmonth, support]                                                     |\n",
      "|#biden        |1380523377279041538|amp have lost control of supporting 's with trillions in has reached a terminal state                             |[amp, have, lost, control, of, supporting, 's, with, trillions, in, has, reached, a, terminal, state]                             |[amp, have, lost, control, of, supporting, 's, with, trillion, in, ha, reached, a, terminal, state]                               |[lost, control, supporting, trillion, reached, terminal, state]                                               |\n",
      "|#biden        |1380521435295268871|stocks massively overbought but speculators know so cowed by fear of a dares not even speak it                    |[stocks, massively, overbought, but, speculators, know, so, cowed, by, fear, of, a, dares, not, even, speak, it]                  |[stock, massively, overbought, but, speculator, know, so, cowed, by, fear, of, a, dare, not, even, speak, it]                     |[stock, massively, overbought, speculator, know, cowed, fear, dare, even, speak]                              |\n",
      "+--------------+-------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to get rid of words like v, q, wa\n",
    "dataset2 = dataset2.withColumn(\"filtered2\", f.expr(\"filter(filtered, x -> not(length(x) < 3))\")).where(f.size(f.col(\"filtered2\")) > 0).drop(\"filtered\")\n",
    "dataset2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|         label|           tweet_id|          tweet_text|               words|          lemmatized|           filtered2|         tf_features|     tf_idf_features|labelIndex|\n",
      "+--------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        #china|1381303111222956033|could be a messan...|[could, be, a, me...|[could, be, a, me...|[messan, war, world]|(262144,[60080,24...|(262144,[60080,24...|       2.0|\n",
      "|      #vaccine|1380298538802765827|infectious diseas...|[infectious, dise...|[infectious, dise...|[infectious, dise...|(262144,[17058,21...|(262144,[17058,21...|       1.0|\n",
      "|      #vaccine|1381494016211030016|do u truly trust ...|[do, u, truly, tr...|[do, u, truly, tr...|[truly, trust, sa...|(262144,[13130,62...|(262144,[13130,62...|       1.0|\n",
      "|        #covid|1380766615189479425|         do u use q |     [do, u, use, q]|     [do, u, use, q]|               [use]|(262144,[98717],[...|(262144,[98717],[...|       0.0|\n",
      "|        #biden|1380129673443696640|wants but gets am...|[wants, but, gets...|[want, but, get, ...|[want, get, domin...|(262144,[121166,1...|(262144,[121166,1...|       4.0|\n",
      "|#stopasianhate|1380973739224010757|      queen of asia |   [queen, of, asia]|   [queen, of, asia]|       [queen, asia]|(262144,[17991,29...|(262144,[17991,29...|       3.0|\n",
      "|        #covid|1381746356730101760|whats ur main for...|[whats, ur, main,...|[whats, ur, main,...| [whats, main, form]|(262144,[9781,485...|(262144,[9781,485...|       0.0|\n",
      "|        #biden|1380127152847347714|no one wants to b...|[no, one, wants, ...|[no, one, want, t...|[one, want, buy, ...|(262144,[18176,21...|(262144,[18176,21...|       4.0|\n",
      "|        #covid|1381347661396246531|domain for sale d...|[domain, for, sal...|[domain, for, sal...|[domain, sale, do...|(262144,[39510,10...|(262144,[39510,10...|       0.0|\n",
      "|        #china|1380180497062629376|dm or comment rea...|[dm, or, comment,...|[dm, or, comment,...|[comment, ready, ...|(262144,[34374,17...|(262144,[34374,17...|       2.0|\n",
      "|        #china|1380177653383237632|let's get started...|[let's, get, star...|[let's, get, star...|[get, started, li...|(262144,[168855,1...|(262144,[168855,1...|       2.0|\n",
      "|        #biden|1380986061078282245|it was a lie our ...|[it, was, a, lie,...|[it, wa, a, lie, ...|         [lie, govt]|(262144,[155923,2...|(262144,[155923,2...|       4.0|\n",
      "|        #biden|1380521381704708098|new platform hypr...|[new, platform, h...|[new, platform, h...|[new, platform, h...|(262144,[56734,83...|(262144,[56734,83...|       4.0|\n",
      "|        #china|1380182109097091072|don't miss out th...|[don't, miss, out...|[don't, miss, out...|[miss, great, opp...|(262144,[172517,2...|(262144,[172517,2...|       2.0|\n",
      "|        #covid|1381753369576894465|the amazing singa...|[the, amazing, si...|[the, amazing, si...|[amazing, singapo...|(262144,[148227,1...|(262144,[148227,1...|       0.0|\n",
      "|        #china|1381634037891047427|   learn holy quran |[learn, holy, quran]|[learn, holy, quran]|[learn, holy, quran]|(262144,[52351,14...|(262144,[52351,14...|       2.0|\n",
      "|        #covid|1379865363953893378|     covid update v |  [covid, update, v]|  [covid, update, v]|     [covid, update]|(262144,[11395,97...|(262144,[11395,97...|       0.0|\n",
      "|        #biden|1380131305090875393|if is booming why...|[if, is, booming,...|[if, is, booming,...|[booming, sent, a...|(262144,[14355,99...|(262144,[14355,99...|       4.0|\n",
      "|        #biden|1380523377279041538|amp have lost con...|[amp, have, lost,...|[amp, have, lost,...|[lost, control, s...|(262144,[19153,84...|(262144,[19153,84...|       4.0|\n",
      "|        #biden|1380521435295268871|stocks massively ...|[stocks, massivel...|[stock, massively...|[stock, massively...|(262144,[18697,71...|(262144,[18697,71...|       4.0|\n",
      "+--------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing and IDF\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, IDF, HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered2\", outputCol=\"tf_features\")\n",
    "idf = IDF(inputCol = \"tf_features\", outputCol = \"tf_idf_features\")\n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"labelIndex\")\n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(dataset2)\n",
    "dataset2= pipelineFit.transform(dataset2)\n",
    "dataset2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 14718\n",
      "Test Dataset Count: 6426\n"
     ]
    }
   ],
   "source": [
    "(trainingData2, testData2) = dataset2.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData2.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData2.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol = \"labelIndex\", featuresCol = \"tf_idf_features\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData2)\n",
    "lrPredictions = lrModel.transform(testData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|prediction|labelIndex|\n",
      "+----------+----------+\n",
      "|       2.0|       4.0|\n",
      "|       2.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       1.0|       4.0|\n",
      "|       0.0|       4.0|\n",
      "|       0.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       0.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       0.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       0.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       1.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       4.0|       4.0|\n",
      "|       0.0|       4.0|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrPredictions.select(\"prediction\", \"labelIndex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 2469|\n",
      "|       1.0| 1651|\n",
      "|       2.0| 1073|\n",
      "|       3.0|  630|\n",
      "|       4.0|  583|\n",
      "|       5.0|   20|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as col\n",
    "lrPredictions.groupBy(\"prediction\").count().orderBy(f.col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6248054777466542\n",
      "Test Error = 0.375195 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(lrPredictions)\n",
    "print(accuracy)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 0.04181217555450684\n",
      "label 1: 0.021555179478044\n",
      "label 2: 0.0034482758620689655\n",
      "label 3: 0.0006309148264984228\n",
      "label 4: 0.0018764659890539485\n",
      "label 5: 0.0\n",
      "True positive rate by label:\n",
      "label 0: 0.9682270191608052\n",
      "label 1: 0.9553136637066743\n",
      "label 2: 0.954031117397454\n",
      "label 3: 0.9421000981354269\n",
      "label 4: 0.91701244813278\n",
      "label 5: 0.7967741935483871\n",
      "Precision by label:\n",
      "label 0: 0.9001127395715897\n",
      "label 1: 0.9323455409561084\n",
      "label 2: 0.9850310332238043\n",
      "label 3: 0.995850622406639\n",
      "label 4: 0.9866071428571429\n",
      "label 5: 1.0\n",
      "Recall by label:\n",
      "label 0: 0.9682270191608052\n",
      "label 1: 0.9553136637066743\n",
      "label 2: 0.954031117397454\n",
      "label 3: 0.9421000981354269\n",
      "label 4: 0.91701244813278\n",
      "label 5: 0.7967741935483871\n",
      "F-measure by label:\n",
      "label 0: 0.9329282542650151\n",
      "label 1: 0.94368986983588\n",
      "label 2: 0.9692832764505119\n",
      "label 3: 0.968229954614221\n",
      "label 4: 0.9505376344086022\n",
      "label 5: 0.8868940754039497\n",
      "Accuracy: 0.9484984372876749\n",
      "FPR: 0.017821448997847135\n",
      "TPR: 0.948498437287675\n",
      "F-measure: 0.948691670318831\n",
      "Precision: 0.9507659335902745\n",
      "Recall: 0.948498437287675\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create both evaluators\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\")\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = lrPredictions.select(\"prediction\", \"labelIndex\")\n",
    "\n",
    "# Get metrics\n",
    "acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "auc = evaluator.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6261994655603456"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorMulti.evaluate(predictionAndTarget) #gives F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.6248054777466542\n",
      "F1 score: 0.6261994655603456\n",
      "Weighted Precision: 0.6647573810465675\n",
      "Weighted Recall: 0.6248054777466543\n",
      "AUC: 0.6248054777466542\n"
     ]
    }
   ],
   "source": [
    "print(\"ACC: %s\" % acc)\n",
    "print(\"F1 score: %s\" % f1)\n",
    "print(\"Weighted Precision: %s\" % weightedPrecision)\n",
    "print(\"Weighted Recall: %s\" % weightedRecall)\n",
    "print(\"AUC: %s\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegressionTrainingSummary' object has no attribute 'roc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-dd21958161cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainingSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"areaUnderROC: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mareaUnderROC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegressionTrainingSummary' object has no attribute 'roc'"
     ]
    }
   ],
   "source": [
    "# this part does not work for now\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "# objectiveHistory = trainingSummary.objectiveHistory\n",
    "# print(\"objectiveHistory:\")\n",
    "# for objective in objectiveHistory:\n",
    "#     print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show(5)\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head(5)\n",
    "# bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "#     .select('threshold').head()['threshold']\n",
    "# lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "6 X 262144 CSRMatrix\n",
      "(0,0) -0.0143\n",
      "(0,7) -0.0291\n",
      "(0,19) -0.0042\n",
      "(0,51) 0.0425\n",
      "(0,72) -0.012\n",
      "(0,86) -0.0342\n",
      "(0,96) -0.0095\n",
      "(0,108) -0.0197\n",
      "(0,134) -0.0117\n",
      "(0,135) 0.071\n",
      "(0,143) 0.0479\n",
      "(0,170) 0.0683\n",
      "(0,172) -0.0273\n",
      "(0,204) -0.0167\n",
      "(0,207) 0.044\n",
      "(0,216) -0.019\n",
      "..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x7fbbad564f70>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<map at 0x7fbbad564f70>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_temp = lrPredictions.select(\"labelIndex\").groupBy(\"labelIndex\").count().sort('count', ascending=False).toPandas()\n",
    "class_temp = class_temp[\"labelIndex\"].values.tolist()\n",
    "class_names = map(str, class_temp)\n",
    "print(class_names)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1253,  383,   85,   22,   40,    0],\n",
       "       [ 466,  947,   41,   11,   24,    0],\n",
       "       [ 270,  119,  786,   14,   53,    0],\n",
       "       [ 193,   76,   41,  568,   11,    0],\n",
       "       [ 223,  110,   97,   15,  442,    1],\n",
       "       [  64,   16,   23,    0,   13,   19]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = lrPredictions.select(\"labelIndex\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "y_pred = lrPredictions.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "cnf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index To String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "converter = IndexToString(inputCol=\"labelIndex\", outputCol=\"labelOriginal\")\n",
    "converted = converter.transform(lrPredictions)\n",
    "converterPred = IndexToString(inputCol='prediction',outputCol='labelPredicted',labels=pipelineFit.stages[2].labels)\n",
    "model = converterPred.transform(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
